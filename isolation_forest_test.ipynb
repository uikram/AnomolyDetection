{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import math\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'all_combined_EMA2227A10005.csv'\n",
    "df = pd.read_csv('data/'+filename)\n",
    "df = df.dropna(subset='cycle_time')\n",
    "# df = df.rename(columns={'CT': 'cycle_time', 'Shot Time': 'shot_time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset='cycle_time')\n",
    "df = df[['shot_time','cycle_time']]\n",
    "data = ['cycle_time']\n",
    "df['cycle_time'] = df['cycle_time'].apply(math.floor)\n",
    "if_model = IsolationForest(contamination='auto',random_state=42)\n",
    "if_model.fit(df[data])\n",
    "df['anomoly_score'] = if_model.decision_function(df[data])\n",
    "df['anomoly_score_inverse']=if_model.score_samples(df[data])\n",
    "# df['anomoly_score'] = df['anomoly_score']+1\n",
    "df['anomoly_score_inverse'] = df['anomoly_score_inverse']+1\n",
    "df['output'] = if_model.predict(df[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply to each row\n",
    "def assign_anomaly_score(row):\n",
    "    if row['anomoly_score_inverse']<  0.3:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Apply the function to create the new column\n",
    "df['output_anomaly_score'] = df.apply(assign_anomaly_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['gradient_fwd'] = df['cycle_time'].diff(1)\n",
    "# df['gradient_bck'] = df['cycle_time'].diff(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gradient_bck'] = df['cycle_time'].diff(-1) # This will fill the first value with zero, because the difference is x = x[i] - x[i+1]\n",
    "df['gradient_bck'] = df['gradient_bck'].abs()\n",
    "df['gradient_fwd'] = df['cycle_time'].diff()\n",
    "df['gradient_fwd'] = df['gradient_fwd'].abs()\n",
    "tolerance = 5\n",
    "df.loc[ (df['gradient_fwd'] >= tolerance) & (df['gradient_bck'] >= tolerance), 'output_anomaly_score'] = -1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR CONCATENATED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uikra\\AppData\\Local\\Temp\\ipykernel_22772\\3559514974.py:9: DtypeWarning:\n",
      "\n",
      "Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get list of files in folder\n",
    "folder_path = 'data'  # Change this to your folder path\n",
    "dfs = []  # List to hold DataFrames from each file\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Assuming all files are CSVs\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read CSV into DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append DataFrame to list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop rows with NaN values in 'cycle_time' column\n",
    "combined_df = combined_df.dropna(subset=['cycle_time'])\n",
    "combined_df.to_csv('combined_data.csv')\n",
    "# Select only 'shot_time' and 'cycle_time' columns\n",
    "combined_df = combined_df[['shot_time', 'cycle_time','COUNTER_ID']]\n",
    "\n",
    "# Apply floor function to 'cycle_time' column\n",
    "combined_df['cycle_time'] = combined_df['cycle_time'].apply(math.floor)\n",
    "\n",
    "# Define features\n",
    "data = ['cycle_time']\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "# if_model = IsolationForest(contamination='auto', random_state=42)\n",
    "if_model = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Fit model\n",
    "if_model.fit(combined_df[data])\n",
    "\n",
    "# Calculate anomaly scores\n",
    "combined_df['anomoly_score'] = if_model.decision_function(combined_df[data])\n",
    "combined_df['anomoly_score_inverse'] = if_model.score_samples(combined_df[data])\n",
    "combined_df['anomoly_score_inverse'] = combined_df['anomoly_score_inverse'] + 1\n",
    "\n",
    "# Predict anomalies\n",
    "combined_df['output'] = if_model.predict(combined_df[data])\n",
    "\n",
    "# Apply the function to create the new column 'output_anomaly_score'\n",
    "combined_df['output_anomaly_score'] = combined_df.apply(assign_anomaly_score, axis=1)\n",
    "post_preprocessing = False\n",
    "if post_preprocessing == True:\n",
    "    combined_df['gradient_bck'] = combined_df['cycle_time'].diff(-1) # This will fill the first value with zero, because the difference is x = x[i] - x[i+1]\n",
    "    combined_df['gradient_bck'] = combined_df['gradient_bck'].abs()\n",
    "    combined_df['gradient_fwd'] = combined_df['cycle_time'].diff()\n",
    "    combined_df['gradient_fwd'] = combined_df['gradient_fwd'].abs()\n",
    "\n",
    "    tolerance = 5\n",
    "    combined_df.loc[ (combined_df['gradient_fwd'] >= tolerance) & (combined_df['gradient_bck'] >= tolerance), 'output'] = -1\n",
    "\n",
    "    threhold = 2\n",
    "    combined_df.loc[ (combined_df['gradient_fwd'] <= threhold) & (combined_df['gradient_bck'] <= threhold), 'output'] = 1\n",
    "\n",
    "# combined_df.loc[ (combined_df['gradient_fwd'] <= threhold) & (combined_df['gradient_bck'] <= threhold)& (combined_df['output'] == -1), 'output'] = 1\n",
    "# Save result to a new CSV file\n",
    "result_folder = 'result'  # Folder to save result\n",
    "os.makedirs(result_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "result_file_path = os.path.join(result_folder, 'combined_result_iftihar.csv')\n",
    "combined_df.to_csv(result_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uikra\\AppData\\Local\\Temp\\ipykernel_22772\\2910563235.py:19: DtypeWarning:\n",
      "\n",
      "Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n",
      "Generating Files:   0%|          | 22/11760 [03:45<13:36:50,  4.18s/combination]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed filename : combination_cont0.1_est50_samp10_feat2_bootstrapTrue_warmstartTrue\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_feat2_bootstrapTrue_warmstartFalse\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_feat2_bootstrapFalse_warmstartTrue\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_feat2_bootstrapFalse_warmstartFalse\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_feat3_bootstrapTrue_warmstartTrue\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_feat3_bootstrapTrue_warmstartFalse\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_feat3_bootstrapFalse_warmstartTrue\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_feat3_bootstrapFalse_warmstartFalse\n",
      "Error -> max_features must be <= n_features\n",
      "Failed filename : combination_cont0.1_est50_samp10_featauto_bootstrapTrue_warmstartTrue\n",
      "Error -> The 'max_features' parameter of IsolationForest must be an instance of 'int' or a float in the range (0.0, 1.0]. Got 'auto' instead.\n",
      "Failed filename : combination_cont0.1_est50_samp10_featauto_bootstrapTrue_warmstartFalse\n",
      "Error -> The 'max_features' parameter of IsolationForest must be an instance of 'int' or a float in the range (0.0, 1.0]. Got 'auto' instead.\n",
      "Failed filename : combination_cont0.1_est50_samp10_featauto_bootstrapFalse_warmstartTrue\n",
      "Error -> The 'max_features' parameter of IsolationForest must be an instance of 'int' or a float in the range (0.0, 1.0]. Got 'auto' instead.\n",
      "Failed filename : combination_cont0.1_est50_samp10_featauto_bootstrapFalse_warmstartFalse\n",
      "Error -> The 'max_features' parameter of IsolationForest must be an instance of 'int' or a float in the range (0.0, 1.0]. Got 'auto' instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Files:   0%|          | 33/11760 [05:06<30:17:39,  9.30s/combination]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Calculate anomaly scores\u001b[39;00m\n\u001b[0;32m     76\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomoly_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m if_model\u001b[38;5;241m.\u001b[39mdecision_function(combined_df[data])\n\u001b[1;32m---> 77\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomoly_score_inverse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mif_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomoly_score_inverse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomoly_score_inverse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Predict anomalies\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:433\u001b[0m, in \u001b[0;36mIsolationForest.score_samples\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m    431\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtree_dtype, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:445\u001b[0m, in \u001b[0;36mIsolationForest._score_samples\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    442\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Take the opposite of the scores as bigger is better (here less abnormal)\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_chunked_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:475\u001b[0m, in \u001b[0;36mIsolationForest._compute_chunked_score_samples\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    471\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_samples, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# compute score on the slices of test samples:\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     scores[sl] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43msl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:504\u001b[0m, in \u001b[0;36mIsolationForest._compute_score_samples\u001b[1;34m(self, X, subsample_features)\u001b[0m\n\u001b[0;32m    500\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m X[:, features] \u001b[38;5;28;01mif\u001b[39;00m subsample_features \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m    502\u001b[0m     leaves_index \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mapply(X_subset, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 504\u001b[0m     depths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_path_lengths[tree_idx][leaves_index]\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_average_path_length_per_tree[tree_idx][leaves_index]\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    508\u001b[0m     )\n\u001b[0;32m    509\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m*\u001b[39m average_path_length_max_samples\n\u001b[0;32m    510\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# For a single training sample, denominator and depth are 0.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# Therefore, we set the score manually to 1.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import os\n",
    "import gc  # Importing garbage collection module\n",
    "from itertools import product\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# Get list of files in folder\n",
    "folder_path = 'data'  # Change this to your folder path\n",
    "dfs = []  # List to hold DataFrames from each file\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Assuming all files are CSVs\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read CSV into DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append DataFrame to list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop rows with NaN values in 'cycle_time' column\n",
    "combined_df = combined_df.dropna(subset=['cycle_time'])\n",
    "combined_df.to_csv('combined_data.csv')\n",
    "# Select only 'shot_time' and 'cycle_time' columns\n",
    "combined_df = combined_df[['shot_time', 'cycle_time','COUNTER_ID']]\n",
    "\n",
    "# Apply floor function to 'cycle_time' column\n",
    "combined_df['cycle_time'] = combined_df['cycle_time'].apply(math.floor)\n",
    "\n",
    "# Define features\n",
    "data = ['cycle_time']\n",
    "from tqdm import tqdm\n",
    "\n",
    "grid_search_dict = {\n",
    "    'cont_parm': [0.1, 0.2, 0.3, 0.4, 0.5, 'auto'],\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 'auto'],\n",
    "    'max_samples': [10, 20, 30, 40, 50, 60, 100, 150, 200, 'auto'],\n",
    "    'max_features': [1, 0.5, 0.8, 1, 2, 3, 'auto'],\n",
    "    'bootstrap': [True, False],\n",
    "    'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "param_combinations = list(product(*grid_search_dict.values()))\n",
    "\n",
    "for comb in tqdm(param_combinations, desc=\"Generating Files\", unit=\"combination\"):\n",
    "    cont_parm, n_estimators, max_samples, max_features, bootstrap, warm_start = comb\n",
    "    filename = f\"combination_cont{cont_parm}_est{n_estimators}_samp{max_samples}_feat{max_features}_bootstrap{bootstrap}_warmstart{warm_start}\"\n",
    "\n",
    "    try:\n",
    " \n",
    "\n",
    "\n",
    "        # Initialize Isolation Forest model\n",
    "        # if_model = IsolationForest(contamination='auto', random_state=42)\n",
    "        # if_model = IsolationForest(contamination=cont_parm, random_state=42)\n",
    "\n",
    "        if_model = IsolationForest(n_estimators=n_estimators, \n",
    "                                   max_samples=max_samples, \n",
    "                                   contamination=cont_parm, \n",
    "                                   max_features=max_features,\n",
    "                                    bootstrap=bootstrap, \n",
    "                                    random_state=12, \n",
    "                                    warm_start=warm_start)\n",
    "        \n",
    "\n",
    "        # Fit model\n",
    "        if_model.fit(combined_df[data])\n",
    "\n",
    "        # Calculate anomaly scores\n",
    "        combined_df['anomoly_score'] = if_model.decision_function(combined_df[data])\n",
    "        combined_df['anomoly_score_inverse'] = if_model.score_samples(combined_df[data])\n",
    "        combined_df['anomoly_score_inverse'] = combined_df['anomoly_score_inverse'] + 1\n",
    "\n",
    "        # Predict anomalies\n",
    "        combined_df[f'output_{cont_parm}'] = if_model.predict(combined_df[data])\n",
    "\n",
    "        # Apply the function to create the new column 'output_anomaly_score'\n",
    "        combined_df[f'output_anomaly_score_{cont_parm}'] = combined_df.apply(assign_anomaly_score, axis=1)\n",
    "        # combined_df.loc[ (combined_df['gradient_fwd'] <= threhold) & (combined_df['gradient_bck'] <= threhold)& (combined_df['output'] == -1), 'output'] = 1\n",
    "        # Save result to a new CSV file\n",
    "        result_folder = 'result'  # Folder to save result\n",
    "        os.makedirs(result_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "        result_file_path = os.path.join(result_folder, f'{filename}.csv')\n",
    "        combined_df.to_csv(result_file_path, index=False)\n",
    "        # Perform garbage collection\n",
    "        gc.collect()\n",
    "        post_preprocessing = False\n",
    "        if post_preprocessing == True:\n",
    "            combined_df['gradient_bck'] = combined_df['cycle_time'].diff(-1) # This will fill the first value with zero, because the difference is x = x[i] - x[i+1]\n",
    "            combined_df['gradient_bck'] = combined_df['gradient_bck'].abs()\n",
    "            combined_df['gradient_fwd'] = combined_df['cycle_time'].diff()\n",
    "            combined_df['gradient_fwd'] = combined_df['gradient_fwd'].abs()\n",
    "\n",
    "            tolerance = 5\n",
    "            combined_df.loc[ (combined_df['gradient_fwd'] >= tolerance) & (combined_df['gradient_bck'] >= tolerance), 'output'] = -1\n",
    "\n",
    "            threhold = 2\n",
    "            combined_df.loc[ (combined_df['gradient_fwd'] <= threhold) & (combined_df['gradient_bck'] <= threhold), 'output'] = 1\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed filename : {filename}')\n",
    "        print('Error ->',e)\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iftikhar ki khushi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_differences(df, column_name):\n",
    "    for i in range(1, 6):  # Calculate differences up to the fifth order\n",
    "        # Forward differences\n",
    "        df[f'diff_fwd_{i}'] = df[column_name].diff(periods=-i)\n",
    "        df[f'diff_fwd_{i}'] = df[f'diff_fwd_{i}'].abs().round(2)\n",
    "\n",
    "        # Backward differences\n",
    "        df[f'diff_bck_{i}'] = df[column_name].diff(periods=i)\n",
    "        df[f'diff_bck_{i}'] = df[f'diff_bck_{i}'].abs().round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_forward_columns(df, n):\n",
    "    for i in range(1, n+1):\n",
    "        df[f'output_fwd_{i}'] = df['output'].shift(-i)\n",
    "    return df\n",
    "\n",
    "def create_backward_columns(df, n):\n",
    "    for i in range(1, n+1):\n",
    "        df[f'output_bck_{i}'] = df['output'].shift(i)\n",
    "    return df\n",
    "\n",
    "def create_five_shots(row):\n",
    "    fwd_columns = [f'diff_fwd_{i}' for i in range(1, 6)]\n",
    "    fwd_output_columns = [f'output_fwd_{i}' for i in range(1, 6)]\n",
    "    bck_columns = [f'diff_bck_{i}' for i in range(1, 6)]\n",
    "    bck_output_columns = [f'output_bck_{i}' for i in range(1, 6)]\n",
    "    \n",
    "    fwd_values = [(row[fwd_columns[i]], row[fwd_output_columns[i]]) for i in range(5)]\n",
    "    bck_values = [(row[bck_columns[i]], row[bck_output_columns[i]]) for i in range(5)]\n",
    "    \n",
    "    return fwd_values + bck_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/all_combined_EMA2227A10005.csv')\n",
    "df = df.dropna(subset='cycle_time')\n",
    "df = df[['shot_time','cycle_time']]\n",
    "df['output'] = -1\n",
    "df = calculate_differences(df, 'cycle_time')\n",
    "df = create_feature_column(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uikra\\AppData\\Local\\Temp\\ipykernel_22772\\3824538710.py:6: DtypeWarning:\n",
      "\n",
      "Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'data'\n",
    "dfs = [] \n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Assuming all files are CSVs\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop rows with NaN values in 'cycle_time' column\n",
    "combined_df = combined_df.dropna(subset=['cycle_time'])\n",
    "\n",
    "# Select only 'shot_time' and 'cycle_time' columns\n",
    "combined_df = combined_df[['shot_time', 'cycle_time','COUNTER_ID']]\n",
    "\n",
    "# Apply floor function to 'cycle_time' column\n",
    "combined_df['cycle_time'] = combined_df['cycle_time'].apply(math.floor)\n",
    "\n",
    "# Define features\n",
    "data = ['cycle_time']\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "if_model = IsolationForest(contamination='auto', random_state=42)\n",
    "\n",
    "# Fit model\n",
    "if_model.fit(combined_df[data])\n",
    "\n",
    "# Calculate anomaly scores\n",
    "combined_df['anomoly_score'] = if_model.decision_function(combined_df[data])\n",
    "combined_df['anomoly_score_inverse'] = if_model.score_samples(combined_df[data])\n",
    "combined_df['anomoly_score_inverse'] = combined_df['anomoly_score_inverse'] + 1\n",
    "# Predict anomalies\n",
    "combined_df['output'] = if_model.predict(combined_df[data])\n",
    "# Apply the function to create the new column 'output_anomaly_score'\n",
    "combined_df['output_anomaly_score'] = combined_df.apply(assign_anomaly_score, axis=1)\n",
    "combined_df = calculate_differences(combined_df, 'cycle_time')\n",
    "tolerance = 5\n",
    "combined_df.loc[ (combined_df['diff_fwd_1'] >= tolerance) & (combined_df['diff_bck_1'] >= tolerance), 'output'] = -1\n",
    "\n",
    "threhold = 2\n",
    "combined_df.loc[ (combined_df['diff_fwd_1'] <= threhold) & (combined_df['diff_bck_1'] <= threhold), 'output'] = 1\n",
    "\n",
    "# Create forward columns\n",
    "combined_df = create_forward_columns(combined_df, 5)\n",
    "# Create backward columns\n",
    "combined_df = create_backward_columns(combined_df, 5)\n",
    "\n",
    "combined_df['five_shots'] = combined_df.apply(create_five_shots, axis=1)\n",
    "result_folder = 'result'  # Folder to save result\n",
    "os.makedirs(result_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "result_file_path = os.path.join(result_folder, 'combined_result.csv')\n",
    "combined_df.to_csv(result_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combination_cont0.1_est50_samp10_feat0.5_bootstrapFalse_warmstartFalse\n",
      "combination_cont0.1_est50_samp10_feat0.5_bootstrapFalse_warmstartTrue\n",
      "combination_cont0.1_est50_samp10_feat0.5_bootstrapTrue_warmstartFalse\n",
      "combination_cont0.1_est50_samp10_feat0.5_bootstrapTrue_warmstartTrue\n",
      "combination_cont0.1_est50_samp10_feat0.8_bootstrapFalse_warmstartFalse\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[0;32m     53\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_directory, filename)\n\u001b[1;32m---> 54\u001b[0m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[134], line 3\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(file_path, output_directory, name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_file\u001b[39m(file_path, output_directory,name):\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m----> 3\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshot_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshot_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcycle_time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Group by COUNTER_ID\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:468\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mCall array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m result, tz_out \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[1;32m--> 468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtz_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m:\n\u001b[0;32m    469\u001b[0m     unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    470\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m DatetimeTZDtype(tz\u001b[38;5;241m=\u001b[39mtz_out, unit\u001b[38;5;241m=\u001b[39munit)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_file(file_path, output_directory,name):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['shot_time'] = pd.to_datetime(data['shot_time'])\n",
    "    data = data.dropna(subset=['cycle_time'])\n",
    "    # Group by COUNTER_ID\n",
    "    grouped_data = data.groupby('COUNTER_ID')\n",
    "\n",
    "    for counter_id, group_data in grouped_data:\n",
    "        # Set color for markers based on output_anomaly_score column\n",
    "        colors = ['red' if score == -1 else 'rgb(31, 119, 180)' for score in group_data['output_0.1']]\n",
    "\n",
    "        # Add hover text with anomoly_score_inverse\n",
    "        hover_text = [f\"Cycle Time: {ct}<br>Anomaly Score Inverse: {score_inv}\" \n",
    "                      for ct, score_inv in zip(group_data['cycle_time'], group_data['anomoly_score_inverse'])]\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=group_data['shot_time'], y=group_data['cycle_time'], \n",
    "                                 mode='markers+lines', line=dict(color='darkgrey', width=1), \n",
    "                                 marker=dict(symbol='circle', size=8, color=colors),\n",
    "                                 hoverinfo='text',\n",
    "                                 hovertext=hover_text))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f'Cycle Time vs Measurement Date - Counter ID: {counter_id}',\n",
    "            xaxis_title='Measurement Date',\n",
    "            yaxis_title='Cycle Time (seconds)',\n",
    "            template='plotly_white',\n",
    "            plot_bgcolor='lightgrey',\n",
    "            xaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridcolor='white',\n",
    "                tickfont=dict(size=14) \n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridcolor='white',\n",
    "                tickfont=dict(size=14)\n",
    "            )\n",
    "        )\n",
    "        # Output file name based on counter_id\n",
    "        filename = os.path.join(output_directory, f'{name+counter_id}_ct.html')\n",
    "        fig.write_html(filename)\n",
    "\n",
    "output_directory = \"ct_plots\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "input_directory = \"result\"\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        name =filename[:-4]\n",
    "        print(name)\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        process_file(file_path, output_directory,name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABELLING PRODUCTION SEAESON WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def convert_strtime_to_datetime(data):\n",
    "    \"\"\"\n",
    "    Converts the 'TFF' column in the given DataFrame to datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the 'TFF' column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The modified DataFrame with the 'TFF' column converted to datetime format.\n",
    "    \"\"\"\n",
    "    data[\"TFF\"] = pd.to_datetime(data[\"TFF\"], format=\"%Y%m%d%H%M%S\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_time_difference(data):\n",
    "    \"\"\"\n",
    "    Calculates the time difference between consecutive rows in the 'TFF' column of the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the 'TFF' column.\n",
    "\n",
    "    Returns:\n",
    "    Series: The time differences between consecutive rows in hours.\n",
    "    \"\"\"\n",
    "    return (data[\"TFF\"] - data[\"TFF\"].shift(1)).dt.total_seconds() / 3600\n",
    "\n",
    "\n",
    "def get_list_of_prod_seasons(data):\n",
    "    \"\"\"\n",
    "    Splits the given DataFrame into multiple sections based on time differences in the 'TFF' column.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the 'TFF' column.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of DataFrames, where each DataFrame represents a section of consecutive rows with time differences greater than or equal to 4 hours.\n",
    "    \"\"\"\n",
    "    data = data.reset_index()\n",
    "    data = convert_strtime_to_datetime(data)\n",
    "\n",
    "    data[\"time_difference\"] = get_time_difference(data)\n",
    "\n",
    "    break_points = data[\n",
    "        data[\"time_difference\"] >= 2\n",
    "    ].index.values\n",
    "    if break_points.shape[0] > 0:\n",
    "        list_of_sections_indices = np.sort(\n",
    "            np.insert(break_points, len(break_points), data.index.values[-1] + 1)\n",
    "        )\n",
    "        ps = [\n",
    "            data.iloc[:x] if i == 0 else data.iloc[list_of_sections_indices[i - 1] : x]\n",
    "            for i, x in enumerate(list_of_sections_indices)\n",
    "        ]\n",
    "    else:\n",
    "        ps = [data]\n",
    "    return ps\n",
    "def assign_anomaly_score(row):\n",
    "    if row['anomoly_score_inverse']<  0.3:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def convert_to_desired_format(date_str):\n",
    "    # Convert string to datetime object\n",
    "    date_obj = pd.to_datetime(date_str)\n",
    "    # Convert datetime object to string with the desired format\n",
    "    formatted_date = date_obj.strftime(\"%Y%m%d%H%M%S\")\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shot_time</th>\n",
       "      <th>cycle_time</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>anomoly_score_inverse</th>\n",
       "      <th>output_anomaly_score</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-10 19:23:23</td>\n",
       "      <td>68</td>\n",
       "      <td>-0.285656</td>\n",
       "      <td>0.214344</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-10 19:24:31</td>\n",
       "      <td>56</td>\n",
       "      <td>0.100849</td>\n",
       "      <td>0.600849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-10 19:25:28</td>\n",
       "      <td>56</td>\n",
       "      <td>0.100849</td>\n",
       "      <td>0.600849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-10 19:26:25</td>\n",
       "      <td>56</td>\n",
       "      <td>0.100849</td>\n",
       "      <td>0.600849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-10 19:27:21</td>\n",
       "      <td>56</td>\n",
       "      <td>0.100849</td>\n",
       "      <td>0.600849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49657</th>\n",
       "      <td>2024-02-26 23:57:13</td>\n",
       "      <td>59</td>\n",
       "      <td>0.076169</td>\n",
       "      <td>0.576169</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49658</th>\n",
       "      <td>2024-02-26 23:58:13</td>\n",
       "      <td>59</td>\n",
       "      <td>0.076169</td>\n",
       "      <td>0.576169</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49659</th>\n",
       "      <td>2024-02-26 23:59:12</td>\n",
       "      <td>59</td>\n",
       "      <td>0.076169</td>\n",
       "      <td>0.576169</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49660</th>\n",
       "      <td>2024-02-27 00:00:12</td>\n",
       "      <td>59</td>\n",
       "      <td>0.076169</td>\n",
       "      <td>0.576169</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49661</th>\n",
       "      <td>2024-02-27 00:01:11</td>\n",
       "      <td>999</td>\n",
       "      <td>-0.413175</td>\n",
       "      <td>0.086825</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49662 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 shot_time  cycle_time  anomaly_score  anomoly_score_inverse  \\\n",
       "0      2022-05-10 19:23:23          68      -0.285656               0.214344   \n",
       "1      2022-05-10 19:24:31          56       0.100849               0.600849   \n",
       "2      2022-05-10 19:25:28          56       0.100849               0.600849   \n",
       "3      2022-05-10 19:26:25          56       0.100849               0.600849   \n",
       "4      2022-05-10 19:27:21          56       0.100849               0.600849   \n",
       "...                    ...         ...            ...                    ...   \n",
       "49657  2024-02-26 23:57:13          59       0.076169               0.576169   \n",
       "49658  2024-02-26 23:58:13          59       0.076169               0.576169   \n",
       "49659  2024-02-26 23:59:12          59       0.076169               0.576169   \n",
       "49660  2024-02-27 00:00:12          59       0.076169               0.576169   \n",
       "49661  2024-02-27 00:01:11         999      -0.413175               0.086825   \n",
       "\n",
       "       output_anomaly_score  output  \n",
       "0                        -1      -1  \n",
       "1                         1       1  \n",
       "2                         1       1  \n",
       "3                         1       1  \n",
       "4                         1       1  \n",
       "...                     ...     ...  \n",
       "49657                     1       1  \n",
       "49658                     1       1  \n",
       "49659                     1       1  \n",
       "49660                     1       1  \n",
       "49661                    -1      -1  \n",
       "\n",
       "[49662 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_combined_EMA2233M10296.csv')\n",
    "df = df.dropna(subset='cycle_time')\n",
    "df['cycle_time'] = df['cycle_time'].apply(math.floor)\n",
    "df = df[['shot_time','cycle_time']]\n",
    "df['TFF'] = df['shot_time'].apply(lambda x: convert_to_desired_format(x))\n",
    "final_df = pd.DataFrame()\n",
    "# Get list of production seasons\n",
    "prod_seasons = get_list_of_prod_seasons(df)\n",
    "\n",
    "# Process data for each production season\n",
    "for ps_data in prod_seasons:\n",
    "    if_model = IsolationForest(contamination='auto',random_state=42)\n",
    "    if_model.fit(ps_data[['cycle_time']])\n",
    "    # Calculate anomaly scores\n",
    "    ps_data['anomaly_score'] = if_model.decision_function(ps_data[['cycle_time']])\n",
    "    ps_data['anomoly_score_inverse'] = if_model.score_samples(ps_data[['cycle_time']]) + 1\n",
    "    ps_data['output'] = if_model.predict(ps_data[['cycle_time']])\n",
    "    ps_data['output_anomaly_score'] = ps_data.apply(assign_anomaly_score, axis=1)\n",
    "    final_df = pd.concat([final_df, pd.DataFrame(ps_data)], ignore_index=True)\n",
    "final_df= final_df[['shot_time','cycle_time','anomaly_score','anomoly_score_inverse','output_anomaly_score','output']]\n",
    "final_df.to_csv('result/labelled_all_combined_EMA2233M10296_2Hr_Th.csv')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, output_directory):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['shot_time'] = pd.to_datetime(data['shot_time'])\n",
    "    data = data.dropna(subset=['cycle_time'])\n",
    "\n",
    "    # Set color for markers based on output_anomaly_score column\n",
    "    colors = ['red' if score == -1 else 'rgb(31, 119, 180)' for score in data['output']]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=data['shot_time'], y=data['cycle_time'], \n",
    "                             mode='markers+lines', line=dict(color='darkgrey', width=1), \n",
    "                             marker=dict(symbol='circle', size=8, color=colors)))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Cycle Time vs Measurement Date ',\n",
    "        xaxis_title='Measurement Date',\n",
    "        yaxis_title='Cycle Time (seconds)',\n",
    "        template='plotly_white',\n",
    "        plot_bgcolor='lightgrey',\n",
    "        xaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridcolor='white',\n",
    "            tickfont=dict(size=14) \n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridcolor='white',\n",
    "            tickfont=dict(size=14)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Assuming file_path and output_directory are defined\n",
    "    filename = os.path.join(output_directory, '{}_ct.html'.format(os.path.basename(file_path).split('.')[0]))\n",
    "    fig.write_html(filename)\n",
    "\n",
    "output_directory = \"ct_plots\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "input_directory = \"result\"\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        process_file(file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     49\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_directory, filename)\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[118], line 40\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(file_path, output_directory)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Assuming file_path and output_directory are defined\u001b[39;00m\n\u001b[0;32m     39\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ct.html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m---> 40\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\plotly\\basedatatypes.py:3720\u001b[0m, in \u001b[0;36mBaseFigure.write_html\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3605\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3606\u001b[0m \u001b[38;5;124;03mWrite a figure to an HTML file representation\u001b[39;00m\n\u001b[0;32m   3607\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3716\u001b[0m \u001b[38;5;124;03m    Representation of figure as an HTML div string\u001b[39;00m\n\u001b[0;32m   3717\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3718\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mwrite_html(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\plotly\\io\\_html.py:505\u001b[0m, in \u001b[0;36mwrite_html\u001b[1;34m(fig, file, config, auto_play, include_plotlyjs, include_mathjax, post_script, full_html, animation_opts, validate, default_width, default_height, auto_open, div_id)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03mWrite a figure to an HTML file representation\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03m    Representation of figure as an HTML div string\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# Build HTML string\u001b[39;00m\n\u001b[1;32m--> 505\u001b[0m html_str \u001b[38;5;241m=\u001b[39m \u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_play\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_play\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_plotlyjs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_plotlyjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_mathjax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_mathjax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_script\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_script\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43manimation_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manimation_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiv_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiv_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# Check if file is a string\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;66;03m# Use the standard pathlib constructor to make a pathlib object.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\plotly\\io\\_html.py:136\u001b[0m, in \u001b[0;36mto_html\u001b[1;34m(fig, config, auto_play, include_plotlyjs, include_mathjax, post_script, full_html, animation_opts, default_width, default_height, validate, div_id)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_json_plotly\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# ## Validate figure ##\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m fig_dict \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_coerce_fig_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# ## Generate div id ##\u001b[39;00m\n\u001b[0;32m    139\u001b[0m plotdivid \u001b[38;5;241m=\u001b[39m div_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4())\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\plotly\\io\\_utils.py:10\u001b[0m, in \u001b[0;36mvalidate_coerce_fig_to_dict\u001b[1;34m(fig, validate)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasedatatypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseFigure\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fig, BaseFigure):\n\u001b[1;32m---> 10\u001b[0m     fig_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fig, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# This will raise an exception if fig is not a valid plotly figure\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\site-packages\\plotly\\basedatatypes.py:3298\u001b[0m, in \u001b[0;36mBaseFigure.to_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3286\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3287\u001b[0m \u001b[38;5;124;03mConvert figure to a dictionary\u001b[39;00m\n\u001b[0;32m   3288\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3294\u001b[0m \u001b[38;5;124;03mdict\u001b[39;00m\n\u001b[0;32m   3295\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3296\u001b[0m \u001b[38;5;66;03m# Handle data\u001b[39;00m\n\u001b[0;32m   3297\u001b[0m \u001b[38;5;66;03m# -----------\u001b[39;00m\n\u001b[1;32m-> 3298\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[38;5;66;03m# Handle layout\u001b[39;00m\n\u001b[0;32m   3301\u001b[0m \u001b[38;5;66;03m# -------------\u001b[39;00m\n\u001b[0;32m   3302\u001b[0m layout \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout)\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:206\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    204\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m--> 206\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:265\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n\u001b[0;32m    264\u001b[0m     args \u001b[38;5;241m=\u001b[39m (deepcopy(arg, memo) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[1;32m--> 265\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m    267\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:264\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m deep \u001b[38;5;241m=\u001b[39m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n\u001b[1;32m--> 264\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[0;32m    265\u001b[0m y \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n",
      "File \u001b[1;32mc:\\Users\\uikra\\anaconda3\\envs\\ai_st\\lib\\copy.py:138\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    135\u001b[0m     memo \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    137\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(x)\n\u001b[1;32m--> 138\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_nil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _nil:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_file(file_path, output_directory):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['shot_time'] = pd.to_datetime(data['shot_time'])\n",
    "    data = data.dropna(subset=['cycle_time'])\n",
    "\n",
    "    # Set color for markers based on output_anomaly_score column\n",
    "    colors = ['red' if score == -1 else 'rgb(31, 119, 180)' for score in data['output_0.1']]\n",
    "\n",
    "    # Add hover text with anomoly_score_inverse\n",
    "    hover_text = [f\"Cycle Time: {ct}<br>Anomaly Score Inverse: {score_inv}\" \n",
    "                  for ct, score_inv in zip(data['cycle_time'], data['anomoly_score_inverse'])]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=data['shot_time'], y=data['cycle_time'], \n",
    "                             mode='markers+lines', line=dict(color='darkgrey', width=1), \n",
    "                             marker=dict(symbol='circle', size=8, color=colors),\n",
    "                             hoverinfo='text',\n",
    "                             hovertext=hover_text))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Cycle Time vs Measurement Date ',\n",
    "        xaxis_title='Measurement Date',\n",
    "        yaxis_title='Cycle Time (seconds)',\n",
    "        template='plotly_white',\n",
    "        plot_bgcolor='lightgrey',\n",
    "        xaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridcolor='white',\n",
    "            tickfont=dict(size=14) \n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridcolor='white',\n",
    "            tickfont=dict(size=14)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Assuming file_path and output_directory are defined\n",
    "    filename = os.path.join(output_directory, '{}_ct.html'.format(os.path.basename(file_path).split('.')[0]))\n",
    "    fig.write_html(filename)\n",
    "\n",
    "output_directory = \"ct_plots\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "input_directory = \"result\"\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        process_file(file_path, output_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
